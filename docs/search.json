[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "This site hosts documentation, exploratory analysis, and other information related to the job of performing Analyses at Rocky Brands, Inc."
  },
  {
    "objectID": "Research/price_elasticity.html",
    "href": "Research/price_elasticity.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "Price Elasticity is a measurement of changing quantity due to changing prices.\nWe will use \\(\\eta_p\\) to mean price elasticity.\n\n\\(|\\eta_p| < 1\\) is an elastic product\n\n\\(|\\eta_p| \\le 1\\) is an inelastic product\n\nPrice elasticity is defined as\n\\[\\eta_p = \\frac{\\%\\Delta{Q}}{\\%\\Delta{P}}\\] The midpoiont formula for elasticity is more commonly used, and is:\n\\[\n\\%\\Delta{Q}=\\frac{q_2-q_1}{(q_2+q_1)/2} \\\\\n\\%\\Delta{Y}=\\frac{p_2-p_1}{(p_2+p_1)/2}\n\\]\n\n\nGive the following table:\n\nsales = data.frame(price = c(10, 15), qty = c(120, 100))\nsales\n\n  price qty\n1    10 120\n2    15 100\n\n\nWe would calculate \\(\\eta_p\\) as\n\n((sales$qty[2] - sales$qty[1]) / ( (sales$qty[2] + sales$qty[1]) / 2 )) /\n((sales$price[2] - sales$price[1]) / ( (sales$price[2] + sales$price[1]) / 2 ))\n\n[1] -0.4545455\n\n\nAs a function:\n\npct_delta <- function(x1, x2) {\n  (x2 - x1) / mean(c(x2, x1))\n}\n\nelasticity_p <- function(prices, qty) {\n   pct_delta(qty[1], qty[2]) / pct_delta(prices[1], prices[2]) \n}\n\nelasticity_p(sales$price, sales$qty)\n\n[1] -0.4545455\n\n\n\n\n\n\\[\\eta_p = \\beta_{price}(\\frac{\\bar{P}}{\\bar{Q}})\\]\nUsing our above sample:\n\nmodel <- lm(qty ~ price, data = sales)\n\nmodel$coefficients[[\"price\"]] * ( mean(sales$price) / mean(sales$qty) )\n\n[1] -0.4545455\n\n\n\n\n\n\\[\nlog(Y) = a + \\beta{x} \\\\ d(log_y)=\\beta dX \\\\ \\frac{dY}{y} = \\beta{}dX \\\\ 100b = \\frac{\\%\\Delta Y}{Unit\\ \\Delta X}\n\\]\n\nmodel_log <- lm(log(qty) ~ price, data = sales)\n\nmodel_log$coefficients[[\"price\"]]\n\n[1] -0.03646431"
  },
  {
    "objectID": "Documentation/Data Extraction/Data Cubes.html",
    "href": "Documentation/Data Extraction/Data Cubes.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "At the core of a lot of Rocky Brands research is reliance on the old sales cube methods for data aggregation. This setup requires at least familiarity with the MDX language, the use of some python libraries for interacting with the ADOMD client, and a few more steps that are generally unpalatable. This post will go into how to get programmatic access to this MS proprietary tool in R, and cover the basics of the package lehighCube developed to ease the process.\n\n\nYou’ll have to do this on a Windows machine unfortunately, as you’ll need some of the MS drivers installed to interact with the drivers. Your first requirements are the ADOMD and MSOLAPx86 drivers. You can find those here. Additionally you’ll need to use the python library Pyadomd.\n\n\n\nTo make a cube connection, it’s recommended that a project calling such get its own venv to call with. You absolutely can make a global venv for this tool, but for the sake of the author, I just create a new environment to contain each project. If you go thr route of the lehighCube package, you can simply call lehighCube::py_env() and follow the prompts. Otherwise, assuming RStudio you’ll want to go through the reticulate route. I will simply copy and paste the py_env source below as it should be a comprehensive guide to setting up a project folder.\n\nif (interactive() && !reticulate::virtualenv_exists(getwd())) {\n    resp = readline(paste(\"No python env in\", getwd(), \"to install enter y, or press enter to continue /n \\n\"))\n    if (grepl(\"y\", resp)) {\n      reticulate::virtualenv_create(getwd())\n      reticulate::virtualenv_install(getwd(), c(\"pandas\", \"pyadomd\"))\n    } else {\n      message(\"python env required for cube queries. SQL tables unaffected\")\n    }\n  }\n\nIf in RStudio go to Tools > Project Options > Python and select the proper interpreter.\n\n\n\nMDX is cool for what is does, but a pain in the ass for what it doesn’t. The worst part is that Microsoft wants you to pay them to learn how to use the tool they built that you pay them to use, so documentation is sparse. In addition it’s an old technology that has largely fallen out of favor for analytics so the forum posts and the like tend to be very old. If you run into a real problem, I suggest opening Excel and using PowerQuery to build the MDX string, then editing from there.\nSe below for a workflow:\nSelect Manage Data Model in Excel under the Data tab. \nUse Get Extternal Data, Ioften use Existing Connections to find your cube.\n\nGo to Next > Design\nAnd now build your table. Be sure to pick the proper measure group.\n\nHot OK and you have your query string.\nI included a function mdx_string() in the lehighCube package. If given no args it will take the clipboard as input. Otherwise, give it the string. The idea is to clean up and give sensible formatting.\nBelow for before formatting:\n\nSELECT NON EMPTY { [Measures].[Order Quantity] } ON COLUMNS, NON EMPTY { ([Order Date].[Year].[Year].ALLMEMBERS ) } DIMENSION PROPERTIES MEMBER_CAPTION, MEMBER_UNIQUE_NAME ON ROWS FROM ( SELECT ( { [Record Metadata].[Record Type].&[Booked] } ) ON COLUMNS FROM [RBDW-Prod-Model-Sales]) WHERE ( [Record Metadata].[Record Type].&[Booked] ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS\n\nAnd after formatting\n\nSELECT NON EMPTY \n    {\n     [Measures].[Order Quantity] \n    } ON COLUMNS\n, NON EMPTY \n    {\n     \n        (\n        [Order Date].[Year].[Year].ALLMEMBERS \n        ) \n    } ON ROWS FROM \n    (\n     SELECT \n        (\n         \n            {\n             [Record Metadata].[Record Type].&[Booked] \n            } \n        ) ON COLUMNS FROM [RBDW-Prod-Model-Sales]\n    ) WHERE \n    (\n     [Record Metadata].[Record Type].&[Booked] \n    )\n\nThe general flow of a query is to give sets of lists of lists. My advice is to ignore the concept of rows and columns and where clauses. They’re trash. Instead think of dimenstions as grouping variables. i.e.\n\nselect \n  (\n    measures.[sales quantity]\n  )\non 0, \nnon empty \n  {\n    (\n      [Customer Properties].[Customer Name].allmembers\n      , [Customer Properties].[Customer Region].allmembers\n    )\n  }\non 1,\nnon empty\n  {\n    (\n      [Order Date].[Year].allmembers\n    )\n  }\non 2\nfrom [the_cube]"
  },
  {
    "objectID": "Projects/price_elasticity_dashboard.html",
    "href": "Projects/price_elasticity_dashboard.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "Data used are 2 years of Salesforce orders for Rocky, Georgia, and Durango downloaded and parsed from XML. Data are merged with RBDW-Prod-Model-Sales style data, notably brand and marketing group. Standard cost is supplied by the RBDW-Inventory Cube.\nA time series is applied to the sales quantity in order to account for seasonal fluctuations.\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n          brand marketing_collection style year month       date qty\n1: Georgia Boot             Homeland  G108 2020     7 2020-07-01   5\n2: Georgia Boot             Homeland  G108 2020     8 2020-08-01  17\n3: Georgia Boot             Homeland  G108 2020     9 2020-09-01  22\n4: Georgia Boot             Homeland  G108 2020    10 2020-10-01  32\n   mean_price_base mean_price_adjustment mean_price_actual_n_adj flatline\n1:             105            -11.400000                 93.6000 19.88194\n2:             105             -5.250000                 99.7500 28.25694\n3:             105            -13.000000                 92.0000 29.34028\n4:             105             -4.763889                101.4028 31.48611\n      seasonal    trend\n1: -14.8819444 23.20833\n2: -11.2569444 24.75000\n3:  -7.3402778 26.00000\n4:   0.5138889 27.79167\n\n\n\n\n\nTo calculate elasticity, we’ll use a smaller timeframe within the order history rather than the full set. The method used is the \\(\\beta_{price}\\) in an OLS regression divided by the mean quantity / mean price. See this page on price elasticity for more detail.\n\n\n\nx = orders[date >= \"2021-06-01\"]\n\nstyles <- x$style |> unique() |> {\\(x) x[!is.na(x)]}()\n\nxmod = copy(x)[,.(model = list(lm(qty ~ mean_price_base + seasonal, data = .SD)))\n               , .(marketing_collection, brand, style)\n               ][,`:=`(coeff = model[[1]][[\"coefficients\"]][[\"mean_price_base\"]]\n                       , adjr = summary(model[[1]])[[\"adj.r.squared\"]]\n                       )\n                 , .(marketing_collection, brand, style)][]\n\nfor (i in seq_along(styles)) {\n  brand = x[style == styles[i]]$brand[1]\n  mqty = mean(x[style == styles[i]]$qty)\n  mpri = mean(x[style == styles[i]]$mean_price_base)\n  \n  xmod[style == styles[i]\n       , `:=`( mean_price_base = mpri\n               , mean_qty = mqty\n               , elasticity = coeff * mpri / mqty\n       )]\n}\n      \nxmod[]\n\n   marketing_collection        brand style    model      coeff      adjr\n1:             Homeland Georgia Boot  G108 <lm[12]> -0.6816023 0.7474801\n   mean_price_base mean_qty elasticity\n1:        126.7449 23.09524  -3.740581\n\n\n\n\n\nIn addition to the overall elasticity, we want to measure the change in quantity resulting from an elastic relationship. In other words, if we raise the price $1, what is the percent change in quantity we should expect? To do so we’ll calculate that stat and use it as a coefficient for an exponential smoothing forecast.\n\ncomp_elastic <- function(x, price_change = 0, f_periods = 3, as_df = TRUE) {\n  x.ts <- create_ts(x, \"qty\")\n  model = lm(log(qty) ~ mean_price_base + seasonal, data = x)\n  delta_pct_qty = model[[\"coefficients\"]][[\"mean_price_base\"]]\n  \n  x.forecast <- ets(x.ts, model = \"MAM\") |> \n    forecast(f_periods) \n  \n  if (as_df) {\n    x.forecast <- x.forecast|>\n    as.data.table() |>\n    janitor::clean_names()\n  \n  x.forecast[,point_forecast_elastic := point_forecast * (1 + price_change * delta_pct_qty)]\n  x.forecast[,price_change := price_change][,forecast_periods := 1:f_periods]\n  \n  }\n  x.forecast\n}\n\ncomp_elastic(x, as_df = F) |> autoplot() + \n  theme_minimal() + \n  labs(x = \"Periods\", y = \"Qty\")\n\n\n\n\n\n\n\n\nThe above shows forecasting without adjustments to price. Below will demonstrate a single step forecast applying a price change, which will influence forecast order quantities, compared to a model like above that does not use price changes.\n\ny = lapply(seq(-40, 10, 5), \\(i) comp_elastic(x, i, 1)) |>\n  rbindlist()\n\ny |>\n  ggplot(aes(x = price_change)) + \n  geom_line(aes(y = point_forecast, color = \"Static Forecast\")) + \n  geom_line(aes(y = point_forecast_elastic, color = \"Elastic Forecast\")) + \n  theme_minimal() + \n  labs(x = \"Change in Price\", y = \"Forecast\", title = \"Static vs. Elastic Forecasts\")\n\n\n\n\nSo now that we have a model that (perhaps somewhat naively) responds to changes in pricing, we want to determine optimal profitability. To determine profitability we want to find the point at which maximum sales inersect with the highest price.\n\\[\nprofit = (base\\ price+\\Delta P - standard\\ cost) * elastic\\ forecast\n\\] This will account for the effect of price changes \\(\\Delta P\\) on expected demand.\n\nstd <- lehighCube::sql_query(\"select * from style_std_cost where style = 'G108'\")\n\ny[,base_price := x[.N, (mean_price_base)]]\ny[,profit := (base_price + price_change - std$std_cost) * point_forecast_elastic]\n\n\np1 <- y |>\n  ggplot(aes(x = price_change, y = profit)) + \n  geom_point() +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::dollar_format()) + \n  scale_y_continuous(labels = scales::dollar_format())\n\np1\n\n\n\n\nFrom our data we can fit a polynomial regression. Since we have a parabola, we want the coefficients of \\(a^2+bx+c\\) so we can find the x value of the vertex, which will give us the price change with the highest profitability. Since the actual profit is not useful, we need only solve for x.\n\\[vertex = (\\frac{-b}{2a} , \\frac{-D}{4a})\\]\n\nmodel = lm(profit ~ poly(price_change, 2, raw = TRUE, simple = T), y[forecast_periods == 1]) \nco = coefficients(model)\nintercept = -co[2] / (2*co[3])\n\np1 + geom_vline(aes(xintercept = intercept), linetype = 2)\n\n\n\n\nOur intercept gives us the optimal change in price to maximize profitability. -$34.89. We can add this intercept to the current price to get the optimal pricing strategy. $104.11"
  },
  {
    "objectID": "Projects/supply_deprecation.html",
    "href": "Projects/supply_deprecation.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "The supply deprecation model is shortened into 3 functions for model building.\n\n\n\n\nCode\ndep_model_data <- function(sample_size=1000, seed = 100) {\n  message(\"retrieving model sales data\")\n  # retrieve daily order qty from locally hosted sql copy from cubes\n  # as dbplyr objects\n  conn <- sql_conn()\n  tab <- tbl(conn, \"order_qty_by_acct_style_and_date\")\n  tab_act <- tbl(conn, \"acct_attributes\")\n  \n  \n  # set the dates that will be used as params for defining deprecated or live styles\n  dep_date <- as.character(Sys.Date()-180)\n  rec_date <- as.character(Sys.Date()-30)\n  \n  orders <- tab |>\n    group_by(account, style) |>\n    mutate(dep = max(date) <= dep_date) |>\n    ungroup() |>\n    left_join(\n      tab_act |>\n        select(account, key_account, region) #, account_open_date)\n    )\n  \n  \n  sample.dep <- orders |>\n    filter(dep == TRUE) |>\n    select(account, style, dep) |>\n    distinct() |>\n    collect() |>\n    setDT()\n  \n  set.seed(seed)\n  sample.dep <- merge(sample.dep[,.(account, style)][,unique(.SD)][sample(.N, sample_size)]\n                      , orders)\n  \n  \n  sample.liv <- orders |>\n    filter(dep == FALSE & date >= rec_date) |>\n    select(account, style, dep) |>\n    collect() |>\n    setDT()\n  \n  set.seed(seed)\n  sample.liv <- merge(sample.liv[,.(account, style)][,unique(.SD)][sample(.N, sample_size)]\n                      , orders)\n  \n  \n  DBI::dbDisconnect(conn)\n  \n  # concatenate two tables and clean fields up a bit. We'll make some\n  # calculations to use in features here too. \n  sample_data <- rbind(sample.dep, sample.liv)\n  sample_data[, date := lubridate::as_date(date)]\n  sample_data[,`:=`(qty_frac = {qty / sum(qty)}\n                    , t_dep = {as.integer(date - max(date))}\n                    , uid = paste(account, style)\n  )\n  , .(account, style)\n  ]\n  sample_data[,rollsum := runner::sum_run(qty, 7, idx = date, na_pad = F), uid]\n  # sample_data[,account_open_date := as.numeric(Sys.Date() - as.Date(account_open_date))%/%365]\n  \n  message(\"completed pull of sales data\")\n  \n  sample_data[]\n}\n\n\nThis function retrieves and creates sample data to be used in a model. The function takes two parameters, sample_size and seed, with default values of 1000 and 100 respectively. It then connects to a SQL copy of cubes and retrieves two tables, order_qty_by_acct_style_and_date and acct_attributes. It then sets two dates, dep_date and rec_date, which will be used to define deprecated or live styles. It then retrieves orders from the first table, groups them by account and style, and adds the dep column, which will be used to filter out the deprecated styles. It then left joins the second table, selects the relevant fields, and samples the deprecated and live styles. It then disconnects from the SQL copy and creates a new table with the sampled data, adding some calculated fields and a field for rolling sum. Finally, it prints a message that the pull of sales data is complete and returns the sample data.\n\n\n\nThe next function is dep_model_features which takes a sample data set as an argument. It creates several functions that are used to generate various features based on the data. These features include the mean and standard deviation of order breaks, the slope of order breaks and rolling sums, and the ratio of the most recent order to the largest order placed. The code also queries a cube for the previous month’s orders and merges the features with the data from the cube. Finally, it returns the new features.\n\n\n\n\n\nCode\ndep_model_rf <- function(features, seed = 100) {\n  \n  message(\"training model\")\n  \n  warning(nrow(features[is.na(key_account)]), \" removed because of NA in key_account\")\n  features <- features[!is.na(key_account)]\n  \n  set.seed(seed)\n  train <- features[sample(.N, .8*.N, replace = F)]\n  test <- features[!train, on = names(features)]\n  \n  model <- randomForest::randomForest(y = as.factor(train$dep)\n                        , x = train[,!c(\"dep\", \"account\", \"style\", \"uid\")]\n  )\n  \n  confusion <- caret::confusionMatrix(as.factor(test$dep), predict(model, test, type = \"class\"))\n  print(confusion)\n  \n  imp <- data.table(model$importance, keep.rownames = T)[order(-MeanDecreaseGini)]\n  print(imp)\n  \n  invisible(model)\n}\n\n\nThe code above is a function to train a random forest model. It first checks for missing values in the key_account column and removes them from the data. It then randomly samples 80% of the data for the training set and uses the remaining 20% for the test set. The model is trained using the training set and the confusion matrix is printed. Last, the importance of the model is printed in descending order."
  },
  {
    "objectID": "Research/forecasting.html",
    "href": "Research/forecasting.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "This page will cover common forecasting methods. We’ll use the same time series developed in the Time Series post.\n\n\nSimple exponential smoothing is described as\n\\[\\begin{align}\ns_t &= \\alpha{x_t}+(1-\\alpha)s_{t-1} \\\\\ns_t &= s_{t-1}+\\alpha(x_t-s_{t-1})\n\\end{align}\\]\n\n\\(s_t\\) is the smoothed result at \\(_t\\)\n\\(\\alpha\\) is the smoothing factor. \\(0 \\le \\alpha \\le 1\\)\n\n\\(x\\) is the observed value\n\n\nets(gb.ts, model = \"MAM\", damped = TRUE) |> forecast(h = 12) |> plot()"
  },
  {
    "objectID": "Research/time_series.html",
    "href": "Research/time_series.html",
    "title": "RCKY Analysis",
    "section": "",
    "text": "This post is a crash course in time series forecasting via the book of a similar title by Galit Shmueli and Kenneth Lichtendahl1.\n\n\nFor the sake fo this post, and riddled throughout docs in various data projects, the following basic notation will be used to describe forecasting.\n\\[\n\\begin{align}\nt = 1, 2, 3,... &= index\\ denoting\\ periods\\ of\\ interest \\\\\ny_1, y_2, y_3, ..., y_n &= a\\ series\\ of\\ n\\ vales\\ at\\ n\\ points \\\\\nF_t &= the\\ forecasted\\ period\\ at \\ time\\ t \\\\\nF_{t+k} &= the\\ k\\ step\\ ahead\\ when\\ forecasting\\ time\\ t \\\\\ne_t &= forecasting\\ error\n\\end{align}\n\\]\n\n\n\nTime series can be thought of as consisting of 4 components.\n\nLevel: the average values of the series\n\nTrend: The underlying change in level over time\n\nSeasonality: Short term cycles in data\n\nNoise: The random variation within the data\n\nThe goal of most forecasting is to separate the components out and use noise to predict accuracy.\nThere are two types of models that combine components differntly. Additive and multiplicative. The idea is quite simple. An additive time series could be thought of as:\n\\[y_1 = level + trend + seasonality + noise\\]\nwhile multiplicative:\n\\[y_1 = level * trend * seasonality * noise\\]\n\n\nThis image shamelessly copied from Practical Time Series Forecasting with R. The figure in the book is annotated as giving credit to Dr. Jim Flowers of Boise State, but link rot has meant I cannot find the source. Forgive me."
  },
  {
    "objectID": "Research/time_series.html#the-time-series-class",
    "href": "Research/time_series.html#the-time-series-class",
    "title": "RCKY Analysis",
    "section": "The time series class",
    "text": "The time series class\nR has the ts() function which creates objects of class time series from the base stats package, which is very useful for many forecasting methods. To demonstrate we’ll use order quantity for a specific style to create a time series and employ some forecasting methods.\n\nlibrary(data.table)\nlibrary(lehighCube)\nlibrary(ggplot2)\nlibrary(forecast)\n\norders <- sql_query(\"select * from order_qty_by_acct_style_and_date where style = 'GB8012'\")\n\ngb <- orders[,.(qty = sum(qty), dllrs = sum(dllrs))\n         , .(date = lubridate::floor_date(as.Date(date), \"months\"))\n       ][,tidyr::complete(.SD, date = seq.Date(min(date), max(date), \"months\"), fill = list(qty = 0, dllrs = 0))\n        ]\nsetDT(gb)\ngb[,month := month(date)]\nhead(gb)\n\n         date qty  dllrs month\n1: 2020-01-01   5  65.00     1\n2: 2020-02-01   1  15.00     2\n3: 2020-03-01  21 303.75     3\n4: 2020-04-01  18 265.75     4\n5: 2020-05-01  18 120.00     5\n6: 2020-06-01   2  30.00     6\n\n\nwe can see the above data returns a dataframe with dates and quantities. It is important to note the use of tidyr::complete to provide missing data pints. Most cube queries will by default use non empty {} clauses, and so data pulls are likely to need completed, especially for sparse or less common sales.\nTo convert to a time series (ts) we provide the vector of values, in this case qty, the freuency of the observations in a period, in this case 12 for each month of the year, and the start for the period. If observations started in Feb we’d give this value a 2. In our case we’ll take the month of the least observation.\n\ngb.ts <- gb[order(date)]$qty |>\n  ts(frequency = 12, start = gb[order(date)]$month[1])\n\ngb.ts\n\n   Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n1    5    1   21   18   18    2    4   15   20  320  907  544\n2 1148 2014  420  234  297  542  245   82  388  501  754  455\n3  811  382  887  443  312  396  286  323  299  264  646  689\n4  933  127                                                  \n\n\n\nautoplot(gb.ts) + \n  theme_classic() + \n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(x = \"Time (in Preiods)\", y = \"Qty\", title = \"Style GB8012 as Time Series\")"
  },
  {
    "objectID": "Research/time_series.html#decomposition",
    "href": "Research/time_series.html#decomposition",
    "title": "RCKY Analysis",
    "section": "Decomposition",
    "text": "Decomposition\nWe’ve seen above that there are components to a time series such as seasonality, trend, etc. The function decompose() will attempt to separate out the individual components of our time series to be used as we need. To do so, the decompose function creates a trend line, a moving average, and then uses that to calculate seasonal effects. The help ?decompose is pretty comprehensive.\n\ngb.dts = decompose(gb.ts)\ngb.dts |>\n  autoplot() +\n  theme_bw() + \n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(x = \"Time (in Preiods)\", y = \"Qty\")"
  },
  {
    "objectID": "Research/time_series.html#considerations",
    "href": "Research/time_series.html#considerations",
    "title": "RCKY Analysis",
    "section": "Considerations",
    "text": "Considerations\nWe can see that there is a disparity in volume of qty between the first and the remaining periods. The first year of sales data are sparse, with less than 100 orders per, while the remainder see ~500 orders per observational period. This can either be because of changes in marketing, data gathering, etc. But we might be interested in more typical behavior. In addition to shorteningperidos to just 2021 and beyond, we’ll be sure that partial months aren’t gathered. We’d hate our forecast to takea nosedive just because order qty on Feb 15th isn’t fully representative of a month’s orders.\n\ngb.ts2 <- gb[date >= \"2021-01-01\" & date < \"2023-02-01\"]$qty |>\n  ts(frequency = 12, start = gb[date >= \"2021-01-01\" & date < \"2023-02-01\"]$month[1])\n\ndecompose(gb.ts2) |>\n  autoplot() +\n  theme_bw() + \n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(x = \"Time (in Preiods)\", y = \"Qty\")\n\n\n\n\nwe can see a much steadier picture. The remainder, or residuals, have decreased significantly."
  }
]